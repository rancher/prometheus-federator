--- charts-original/values.yaml
+++ charts/values.yaml
@@ -2,568 +2,7 @@
 # This is a YAML-formatted file.
 # Declare variables to be passed into your templates.
 
-# Rancher Monitoring Configuration
-
-## Configuration for prometheus-adapter
-## ref: https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-adapter
-##
-prometheus-adapter:
-  enabled: true
-  prometheus:
-    # Change this if you change the namespaceOverride or nameOverride of prometheus-operator
-    url: http://rancher-monitoring-prometheus.cattle-monitoring-system.svc
-    port: 9090
-
-## RKE PushProx Monitoring
-## ref: https://github.com/rancher/charts/tree/dev-v2.9/packages/rancher-monitoring/rancher-pushprox
-##
-rkeControllerManager:
-  enabled: false
-  metricsPort: 10257 # default to secure port as of k8s >= 1.22
-  component: kube-controller-manager
-  clients:
-    https:
-      enabled: true
-      insecureSkipVerify: true
-      useServiceAccountCredentials: true
-    port: 10011
-    useLocalhost: true
-    nodeSelector:
-      node-role.kubernetes.io/controlplane: "true"
-    tolerations:
-    - effect: "NoExecute"
-      operator: "Exists"
-    - effect: "NoSchedule"
-      operator: "Exists"
-  kubeVersionOverrides:
-  - constraint: "< 1.22"
-    values:
-      metricsPort: 10252 # default to insecure port in k8s < 1.22
-      clients:
-        https:
-          enabled: false
-          insecureSkipVerify: false
-          useServiceAccountCredentials: false
-
-rkeScheduler:
-  enabled: false
-  metricsPort: 10259
-  component: kube-scheduler
-  clients:
-    https:
-      enabled: true
-      insecureSkipVerify: true
-      useServiceAccountCredentials: true
-    port: 10012
-    useLocalhost: true
-    nodeSelector:
-      node-role.kubernetes.io/controlplane: "true"
-    tolerations:
-    - effect: "NoExecute"
-      operator: "Exists"
-    - effect: "NoSchedule"
-      operator: "Exists"
-  kubeVersionOverrides:
-  - constraint: "< 1.23"
-    values:
-      metricsPort: 10251 # default to insecure port in k8s < 1.23
-      clients:
-        https:
-          enabled: false
-          insecureSkipVerify: false
-          useServiceAccountCredentials: false
-
-rkeProxy:
-  enabled: false
-  metricsPort: 10249
-  component: kube-proxy
-  clients:
-    port: 10013
-    useLocalhost: true
-    tolerations:
-    - effect: "NoExecute"
-      operator: "Exists"
-    - effect: "NoSchedule"
-      operator: "Exists"
-
-rkeEtcd:
-  enabled: false
-  metricsPort: 2379
-  component: kube-etcd
-  clients:
-    port: 10014
-    https:
-      enabled: true
-      certDir: /etc/kubernetes/ssl
-      certFile: kube-etcd-*.pem
-      keyFile: kube-etcd-*-key.pem
-      caCertFile: kube-ca.pem
-      seLinuxOptions:
-        # Gives rkeEtcd permissions to read files in /etc/kubernetes/*
-        # Type is defined in https://github.com/rancher/rancher-selinux
-        type: rke_kubereader_t
-    nodeSelector:
-      node-role.kubernetes.io/etcd: "true"
-    tolerations:
-    - effect: "NoExecute"
-      operator: "Exists"
-    - effect: "NoSchedule"
-      operator: "Exists"
-
-rkeIngressNginx:
-  enabled: false
-  metricsPort: 10254
-  component: ingress-nginx
-  clients:
-    port: 10015
-    useLocalhost: true
-    tolerations:
-      - effect: "NoExecute"
-        operator: "Exists"
-      - effect: "NoSchedule"
-        operator: "Exists"
-    nodeSelector:
-      node-role.kubernetes.io/worker: "true"
-
-## k3s PushProx Monitoring
-## ref: https://github.com/rancher/charts/tree/dev-v2.9/packages/rancher-monitoring/rancher-pushprox
-##
-k3sServer:
-  enabled: false
-  metricsPort: 10250
-  component: k3s-server
-  clients:
-    port: 10013
-    useLocalhost: true
-    https:
-      enabled: true
-      useServiceAccountCredentials: true
-      insecureSkipVerify: true
-    rbac:
-      additionalRules:
-      - nonResourceURLs: ["/metrics/cadvisor"]
-        verbs: ["get"]
-      - apiGroups: [""]
-        resources: ["nodes/metrics"]
-        verbs: ["get"]
-    tolerations:
-    - effect: "NoExecute"
-      operator: "Exists"
-    - effect: "NoSchedule"
-      operator: "Exists"
-  serviceMonitor:
-    endpoints:
-    - port: metrics
-      honorLabels: true
-      relabelings:
-      - sourceLabels: [__metrics_path__]
-        targetLabel: metrics_path
-    - port: metrics
-      path: /metrics/cadvisor
-      honorLabels: true
-      relabelings:
-      - sourceLabels: [__metrics_path__]
-        targetLabel: metrics_path
-    - port: metrics
-      path: /metrics/probes
-      honorLabels: true
-      relabelings:
-      - sourceLabels: [__metrics_path__]
-        targetLabel: metrics_path
-
-hardened:
-  k3s:  
-    networkPolicy:
-      enabled: true
-
-## KubeADM PushProx Monitoring
-## ref: https://github.com/rancher/charts/tree/dev-v2.9/packages/rancher-monitoring/rancher-pushprox
-##
-kubeAdmControllerManager:
-  enabled: false
-  metricsPort: 10257
-  component: kube-controller-manager
-  clients:
-    port: 10011
-    useLocalhost: true
-    https:
-      enabled: true
-      useServiceAccountCredentials: true
-      insecureSkipVerify: true
-    nodeSelector:
-      node-role.kubernetes.io/master: ""
-    tolerations:
-    - effect: "NoExecute"
-      operator: "Exists"
-    - effect: "NoSchedule"
-      operator: "Exists"
-
-kubeAdmScheduler:
-  enabled: false
-  metricsPort: 10259
-  component: kube-scheduler
-  clients:
-    port: 10012
-    useLocalhost: true
-    https:
-      enabled: true
-      useServiceAccountCredentials: true
-      insecureSkipVerify: true
-    nodeSelector:
-      node-role.kubernetes.io/master: ""
-    tolerations:
-    - effect: "NoExecute"
-      operator: "Exists"
-    - effect: "NoSchedule"
-      operator: "Exists"
-
-kubeAdmProxy:
-  enabled: false
-  metricsPort: 10249
-  component: kube-proxy
-  clients:
-    port: 10013
-    useLocalhost: true
-    tolerations:
-    - effect: "NoExecute"
-      operator: "Exists"
-    - effect: "NoSchedule"
-      operator: "Exists"
-
-kubeAdmEtcd:
-  enabled: false
-  metricsPort: 2381
-  component: kube-etcd
-  clients:
-    port: 10014
-    useLocalhost: true
-    nodeSelector:
-      node-role.kubernetes.io/master: ""
-    tolerations:
-    - effect: "NoExecute"
-      operator: "Exists"
-    - effect: "NoSchedule"
-      operator: "Exists"
-
-## rke2 PushProx Monitoring
-## ref: https://github.com/rancher/charts/tree/dev-v2.9/packages/rancher-monitoring/rancher-pushprox
-##
-rke2ControllerManager:
-  enabled: false
-  metricsPort: 10257 # default to secure port as of k8s >= 1.22
-  component: kube-controller-manager
-  clients:
-    https:
-      enabled: true
-      insecureSkipVerify: true
-      useServiceAccountCredentials: true
-    port: 10011
-    useLocalhost: true
-    nodeSelector:
-      node-role.kubernetes.io/master: "true"
-    tolerations:
-      - effect: "NoExecute"
-        operator: "Exists"
-      - effect: "NoSchedule"
-        operator: "Exists"
-  kubeVersionOverrides:
-  - constraint: "< 1.22"
-    values:
-      metricsPort: 10252 # default to insecure port in k8s < 1.22
-      clients:
-        https:
-          enabled: false
-          insecureSkipVerify: false
-          useServiceAccountCredentials: false
-
-rke2Scheduler:
-  enabled: false
-  metricsPort: 10259 # default to secure port as of k8s >= 1.22
-  component: kube-scheduler
-  clients:
-    https:
-      enabled: true
-      insecureSkipVerify: true
-      useServiceAccountCredentials: true
-    port: 10012
-    useLocalhost: true
-    nodeSelector:
-      node-role.kubernetes.io/master: "true"
-    tolerations:
-      - effect: "NoExecute"
-        operator: "Exists"
-      - effect: "NoSchedule"
-        operator: "Exists"
-  kubeVersionOverrides:
-  - constraint: "< 1.22"
-    values:
-      metricsPort: 10251 # default to insecure port in k8s < 1.22
-      clients:
-        https:
-          enabled: false
-          insecureSkipVerify: false
-          useServiceAccountCredentials: false
-
-rke2Proxy:
-  enabled: false
-  metricsPort: 10249
-  component: kube-proxy
-  clients:
-    port: 10013
-    useLocalhost: true
-    tolerations:
-      - effect: "NoExecute"
-        operator: "Exists"
-      - effect: "NoSchedule"
-        operator: "Exists"
-
-rke2Etcd:
-  enabled: false
-  metricsPort: 2381
-  component: kube-etcd
-  clients:
-    port: 10014
-    useLocalhost: true
-    nodeSelector:
-      node-role.kubernetes.io/etcd: "true"
-    tolerations:
-      - effect: "NoExecute"
-        operator: "Exists"
-      - effect: "NoSchedule"
-        operator: "Exists"
-
-rke2IngressNginx:
-  enabled: false
-  metricsPort: 10254
-  component: ingress-nginx
-  networkPolicy:
-    enabled: false
-  # in the RKE2 cluster, the ingress-nginx-controller is deployed
-  # as a non-hostNetwork workload starting at the following versions
-  # - >= v1.22.12+rke2r1 < 1.23.0-0
-  # - >= v1.23.9+rke2r1 < 1.24.0-0
-  # - >= v1.24.3+rke2r1 < 1.25.0-0
-  # - >= v1.25.0+rke2r1
-  # As a result we do not need clients and proxies as we can directly create
-  # a service that targets the workload with the given app name
-  namespaceOverride: kube-system
-  clients:
-    enabled: false
-  proxy:
-    enabled: false
-  service:
-    selector:
-      app.kubernetes.io/name: rke2-ingress-nginx
-  kubeVersionOverrides:
-  - constraint: "< 1.21.0-0"
-    values:
-      namespaceOverride: ""
-      clients:
-        enabled: true
-        port: 10015
-        useLocalhost: true
-        tolerations:
-          - effect: "NoExecute"
-            operator: "Exists"
-          - effect: "NoSchedule"
-            operator: "Exists"
-        affinity:
-          podAffinity:
-            requiredDuringSchedulingIgnoredDuringExecution:
-              - labelSelector:
-                  matchExpressions:
-                    - key: "app.kubernetes.io/component"
-                      operator: "In"
-                      values:
-                        - "controller"
-                topologyKey: "kubernetes.io/hostname"
-                namespaces:
-                  - "kube-system"
-        # in the RKE2 cluster, the ingress-nginx-controller is deployed as
-        # a DaemonSet with 1 pod when RKE2 version is < 1.21.0-0
-        deployment:
-          enabled: false
-      proxy:
-        enabled: true
-      service:
-        selector: false
-  - constraint: ">= 1.21.0-0 < 1.22.12-0"
-    values:
-      namespaceOverride: ""
-      clients:
-        enabled: true
-        port: 10015
-        useLocalhost: true
-        tolerations:
-          - effect: "NoExecute"
-            operator: "Exists"
-          - effect: "NoSchedule"
-            operator: "Exists"
-        affinity:
-          podAffinity:
-            requiredDuringSchedulingIgnoredDuringExecution:
-              - labelSelector:
-                  matchExpressions:
-                    - key: "app.kubernetes.io/component"
-                      operator: "In"
-                      values:
-                        - "controller"
-                topologyKey: "kubernetes.io/hostname"
-                namespaces:
-                  - "kube-system"
-        # in the RKE2 cluster, the ingress-nginx-controller is deployed as
-        # a hostNetwork Deployment with 1 pod when RKE2 version is >= 1.21.0-0
-        deployment:
-          enabled: true
-          replicas: 1
-      proxy:
-        enabled: true
-      service:
-        selector: false
-  - constraint: ">= 1.23.0-0 < v1.23.9-0"
-    values:
-      namespaceOverride: ""
-      clients:
-        enabled: true
-        port: 10015
-        useLocalhost: true
-        tolerations:
-          - effect: "NoExecute"
-            operator: "Exists"
-          - effect: "NoSchedule"
-            operator: "Exists"
-        affinity:
-          podAffinity:
-            requiredDuringSchedulingIgnoredDuringExecution:
-              - labelSelector:
-                  matchExpressions:
-                    - key: "app.kubernetes.io/component"
-                      operator: "In"
-                      values:
-                        - "controller"
-                topologyKey: "kubernetes.io/hostname"
-                namespaces:
-                  - "kube-system"
-        # in the RKE2 cluster, the ingress-nginx-controller is deployed as
-        # a hostNetwork Deployment with 1 pod when RKE2 version is >= 1.20.0-0
-        deployment:
-          enabled: true
-          replicas: 1
-      proxy:
-        enabled: true
-      service:
-        selector: false
-  - constraint: ">= 1.24.0-0 < v1.24.3-0"
-    values:
-      namespaceOverride: ""
-      clients:
-        enabled: true
-        port: 10015
-        useLocalhost: true
-        tolerations:
-          - effect: "NoExecute"
-            operator: "Exists"
-          - effect: "NoSchedule"
-            operator: "Exists"
-        affinity:
-          podAffinity:
-            requiredDuringSchedulingIgnoredDuringExecution:
-              - labelSelector:
-                  matchExpressions:
-                    - key: "app.kubernetes.io/component"
-                      operator: "In"
-                      values:
-                        - "controller"
-                topologyKey: "kubernetes.io/hostname"
-                namespaces:
-                  - "kube-system"
-        # in the RKE2 cluster, the ingress-nginx-controller is deployed as
-        # a hostNetwork Deployment with 1 pod when RKE2 version is >= 1.20.0-0
-        deployment:
-          enabled: true
-          replicas: 1
-      proxy:
-        enabled: true
-      service:
-        selector: false
-
-
-
-## Additional PushProx Monitoring
-## ref: https://github.com/rancher/charts/tree/dev-v2.9/packages/rancher-monitoring/rancher-pushprox
-##
-
-# hardenedKubelet can only be deployed if kubelet.enabled=true
-# If enabled, it replaces the ServiceMonitor deployed by the default kubelet option with a 
-# PushProx-based exporter that does not require a host port to be open to scrape metrics.
-hardenedKubelet:
-  enabled: false
-  metricsPort: 10250
-  component: kubelet
-  clients:
-    port: 10015
-    useLocalhost: true
-    https:
-      enabled: true
-      useServiceAccountCredentials: true
-      insecureSkipVerify: true
-    rbac:
-      additionalRules:
-      - nonResourceURLs: ["/metrics/cadvisor"]
-        verbs: ["get"]
-      - apiGroups: [""]
-        resources: ["nodes/metrics"]
-        verbs: ["get"]
-    tolerations:
-      - effect: "NoExecute"
-        operator: "Exists"
-      - effect: "NoSchedule"
-        operator: "Exists"
-  serviceMonitor:
-    endpoints:
-    - port: metrics
-      honorLabels: true
-      relabelings:
-      - sourceLabels: [__metrics_path__]
-        targetLabel: metrics_path
-    - port: metrics
-      path: /metrics/cadvisor
-      honorLabels: true
-      relabelings:
-      - sourceLabels: [__metrics_path__]
-        targetLabel: metrics_path
-    - port: metrics
-      path: /metrics/probes
-      honorLabels: true
-      relabelings:
-      - sourceLabels: [__metrics_path__]
-        targetLabel: metrics_path
-
-# hardenedNodeExporter can only be deployed if nodeExporter.enabled=true
-# If enabled, it replaces the ServiceMonitor deployed by the default nodeExporter with a 
-# PushProx-based exporter that does not require a host port to be open to scrape metrics.
-hardenedNodeExporter:
-  enabled: false
-  metricsPort: 9796
-  component: node-exporter
-  clients:
-    port: 10016
-    useLocalhost: true
-    tolerations:
-      - effect: "NoExecute"
-        operator: "Exists"
-      - effect: "NoSchedule"
-        operator: "Exists"
-
-## Upgrades
-upgrade:
-  ## Run upgrade scripts before an upgrade or rollback via a Job hook
-  enabled: true
-  ## Image to use to run the scripts
-  image:
-    repository: rancher/shell
-    tag: v0.2.1
-
+# Rancher Project Monitoring Configuration
 ## Rancher Monitoring
 ##
 
@@ -582,59 +21,17 @@
   ##
   selector: {}
 
-## Component scraping nginx-ingress-controller
-##
-ingressNginx:
-  enabled: false
-
-  ## The namespace to search for your nginx-ingress-controller
-  ##
-  namespace: ingress-nginx
-  
-  service:
-    port: 9913
-    targetPort: 10254
-    # selector:
-    #   app: ingress-nginx
-  serviceMonitor:
-    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
-    ##
-    interval: "30s"
-
-    ## proxyUrl: URL of a proxy that should be used for scraping.
-    ##
-    proxyUrl: ""
-
-    ## 	metric relabel configs to apply to samples before ingestion.
-    ##
-    metricRelabelings: []
-    # - action: keep
-    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
-    #   sourceLabels: [__name__]
-
-    # 	relabel configs to apply to samples before ingestion.
-    ##
-    relabelings: []
-    # - sourceLabels: [__meta_kubernetes_pod_node_name]
-    #   separator: ;
-    #   regex: ^(.*)$
-    #   targetLabel: nodename
-    #   replacement: $1
-    #   action: replace
-
-# Prometheus Operator Configuration
-
 ## Provide a name in place of kube-prometheus-stack for `app:` labels
 ## NOTE: If you change this value, you must update the prometheus-adapter.prometheus.url
 ##
-nameOverride: "rancher-monitoring"
+nameOverride: "rancher-project-monitoring"
 
 ## Override the deployment namespace
 ## NOTE: If you change this value, you must update the prometheus-adapter.prometheus.url
 ##
-namespaceOverride: "cattle-monitoring-system"
+namespaceOverride: ""
 
-## Provide a k8s version to auto dashboard import script example: kubeTargetVersionOverride: 1.26.6
+## Provide a k8s version to auto dashboard import script example: kubeTargetVersionOverride: 1.16.6
 ##
 kubeTargetVersionOverride: ""
 
@@ -652,66 +49,24 @@
 # scmhash: abc123
 # myLabel: aakkmd
 
-## Install Prometheus Operator CRDs
-##
-crds:
-  enabled: true
-
 ## custom Rules to override "for" and "severity" in defaultRules
 ##
 customRules: {}
-  # AlertmanagerFailedReload:
-  #   for: 3m
-  # AlertmanagerMembersInconsistent:
-  #   for: 5m
-  #   severity: "warning"
 
 ## Create default rules for monitoring the cluster
 ##
 defaultRules:
   create: true
   rules:
-    alertmanager: true
-    etcd: true
-    configReloaders: true
     general: true
-    k8sContainerCpuUsageSecondsTotal: true
-    k8sContainerMemoryCache: true
-    k8sContainerMemoryRss: true
-    k8sContainerMemorySwap: true
-    k8sContainerResource: true
-    k8sContainerMemoryWorkingSetBytes: true
-    k8sPodOwner: true
-    kubeApiserverAvailability: true
-    kubeApiserverBurnrate: true
-    kubeApiserverHistogram: true
-    kubeApiserverSlos: true
-    kubeControllerManager: true
-    kubelet: true
-    kubeProxy: true
-    kubePrometheusGeneral: true
-    kubePrometheusNodeRecording: true
+    prometheus: true
+    alertmanager: true
     kubernetesApps: true
-    kubernetesResources: true
     kubernetesStorage: true
-    kubernetesSystem: true
-    kubeSchedulerAlerting: true
-    kubeSchedulerRecording: true
-    kubeStateMetrics: true
-    network: true
-    node: true
-    nodeExporterAlerting: true
-    nodeExporterRecording: true
-    prometheus: true
-    prometheusOperator: true
-    windows: true
 
   ## Reduce app namespace alert scope
   appNamespacesTarget: ".*"
 
-  ## Set keep_firing_for for all alerts
-  keepFiringFor: ""
-
   ## Labels for default rules
   labels: {}
   ## Annotations for default rules
@@ -757,7 +112,7 @@
     nodeExporterRecording: {}
     prometheus: {}
     prometheusOperator: {}
-
+  
   ## Additional annotations for specific PrometheusRule alerts groups
   additionalRuleGroupAnnotations:
     alertmanager: {}
@@ -793,56 +148,37 @@
     prometheus: {}
     prometheusOperator: {}
 
-  additionalAggregationLabels: []
-
   ## Prefix for runbook URLs. Use this to override the first part of the runbookURLs that is common to all rules.
   runbookUrl: "https://runbooks.prometheus-operator.dev/runbooks"
 
   ## Disabled PrometheusRule alerts
   disabled: {}
-  # KubeAPIDown: true
-  # NodeRAIDDegraded: true
-
-## Deprecated way to provide custom recording or alerting rules to be deployed into the cluster.
-##
-# additionalPrometheusRules: []
-#  - name: my-rule-file
-#    groups:
-#      - name: my_group
-#        rules:
-#        - record: my_record
-#          expr: 100 * my_record
-
-## Provide custom recording or alerting rules to be deployed into the cluster.
-##
-additionalPrometheusRulesMap: {}
-#  rule-name:
-#    groups:
-#    - name: my_group
-#      rules:
-#      - record: my_record
-#        expr: 100 * my_record
 
 ##
 global:
   cattle:
 
     systemDefaultRegistry: ""
-    ## Windows Monitoring
-    ## ref: https://github.com/rancher/charts/tree/dev-v2.5-source/packages/rancher-windows-exporter
-    ##
-    ## Deploys a DaemonSet of Prometheus exporters based on https://github.com/prometheus-community/windows_exporter.
-    ## Every Windows host must have a wins version of 0.1.0+ to use this chart (default as of Rancher 2.5.8).
-    ## To upgrade wins versions on Windows hosts, see https://github.com/rancher/wins/tree/master/charts/rancher-wins-upgrader.
-    ##
-    windows:
-      enabled: false
-  seLinux:
-    enabled: false
+    projectNamespaceSelector: {}
+    projectNamespaces: []
   kubectl:
      repository: rancher/kubectl
      tag: v1.20.2
      pullPolicy: IfNotPresent
+     securityContext:
+       runAsNonRoot: true
+       runAsUser: 1000
+  networkPolicy:
+    # If activated, creates ingress network policies to only allow ingress traffic from workloads within the project.
+    # This only works correctly, if Project Network Isolation is activated for the cluster in Rancher. Otherwise,
+    # Ingress traffic from the nodes and thus from the Kubernetes API will be blocked, which breaks accessing the UIs
+    # through the Rancher/Kubernetes API Proxy in the Rancher UI.
+    limitIngressToProject: false
+    # Custom ingress restrictions. If null and limitIngressToProject=false, all ingress traffic will be allowed.
+    ingress: null
+    # By default, all egress traffic is allowed.
+    egress:
+      - {}
   rbac:
     ## Create RBAC resources for ServiceAccounts and users
     ##
@@ -850,6 +186,18 @@
 
     userRoles:
       ## Create default user ClusterRoles to allow users to interact with Prometheus CRs, ConfigMaps, and Secrets
+      ##
+      ## How does this work?
+      ##
+      ## The operator will watch for all subjects bound to each Kubernetes default ClusterRole in the project registration namespace
+      ## where the ProjectHelmChart that deployed this chart belongs to; if it observes a subject bound to a particular role in
+      ## the project registration namespace (e.g. edit) and if a Role exists that is deployed by this chart with the label
+      ## 'helm.cattle.io/project-helm-chart-role-aggregate-from': '<role, e.g. edit>', it will automaticaly create a RoleBinding
+      ## in the release namespace binding all such subjects to that Role.
+      ##
+      ## Note: while the default behavior is to use the Kubernetes default ClusterRole, the operator deployment (prometheus-federator)
+      ## can be configured to use a different set of ClusterRoles as the source of truth for admin, edit, and view permissions.
+      ##
       create: true
       ## Aggregate default user ClusterRoles into default k8s ClusterRoles
       aggregateToDefaultRoles: true
@@ -868,10 +216,6 @@
       # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'
       # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'
 
-  ## Global image registry to use if it needs to be overriden for some specific use cases (e.g local registries, custom images, ...)
-  ##
-  imageRegistry: "docker.io"
-
   ## Reference to one or more secrets to be used when pulling images
   ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
   ##
@@ -891,29 +235,18 @@
   fluentbit:
     enabled: false
 
-## Configuration for prometheus-windows-exporter
-## ref: https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-windows-exporter
-##
-prometheus-windows-exporter:
-  ## Enable ServiceMonitor and set Kubernetes label to use as a job label
-  ##
-  prometheus:
-    monitor:
-      enabled: true
-      jobLabel: jobLabel
+federate:
+  ## enabled indicates whether to add federation to any Project Prometheus Stacks by default
+  ## If not enabled, no federation will be turned on
+  enabled: true
 
-  releaseLabel: true
+  # Change this to point at all Prometheuses you want all your Project Prometheus Stacks to federate from
+  # By default, this matches the default deployment of Rancher Monitoring
+  targets:
+  - rancher-monitoring-prometheus.cattle-monitoring-system.svc:9090
 
-  ## Set job label to 'windows-exporter' as required by the default Prometheus rules and Grafana dashboards
-  ##
-  podLabels:
-    jobLabel: windows-exporter
-
-  ## Enable memory and container metrics as required by the default Prometheus rules and Grafana dashboards
-  ##
-  config: |-
-    collectors:
-      enabled: '[defaults],memory,container'
+  ## Scrape interval
+  interval: "15s"
 
 ## Configuration for alertmanager
 ## ref: https://prometheus.io/docs/alerting/alertmanager/
@@ -992,19 +325,12 @@
       routes:
       - receiver: 'null'
         matchers:
-          - alertname = "Watchdog"
+          - alertname =~ "InfoInhibitor|Watchdog"
     receivers:
     - name: 'null'
     templates:
     - '/etc/alertmanager/config/*.tmpl'
 
-  ## Alertmanager configuration directives (as string type, preferred over the config hash map)
-  ## stringConfig will be used only, if tplConfig is true
-  ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file
-  ##      https://prometheus.io/webtools/alerting/routing-tree-editor/
-  ##
-  stringConfig: ""
-
   ## Pass the Alertmanager configuration directives through Helm's templating
   ## engine. If the Alertmanager configuration contains Alertmanager templates,
   ## they'll need to be properly escaped so that they are not interpreted by
@@ -1138,53 +464,6 @@
   secret:
     annotations: {}
 
-  # by default the alertmanager secret is not overwritten if it already exists
-    recreateIfExists: false
-
-  ## Configuration for creating an Ingress that will map to each Alertmanager replica service
-  ## alertmanager.servicePerReplica must be enabled
-  ##
-  ingressPerReplica:
-    enabled: false
-
-    # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
-    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
-    # ingressClassName: nginx
-
-    annotations: {}
-    labels: {}
-
-    ## Final form of the hostname for each per replica ingress is
-    ## {{ ingressPerReplica.hostPrefix }}-{{ $replicaNumber }}.{{ ingressPerReplica.hostDomain }}
-    ##
-    ## Prefix for the per replica ingress that will have `-$replicaNumber`
-    ## appended to the end
-    hostPrefix: ""
-    ## Domain that will be used for the per replica ingress
-    hostDomain: ""
-
-    ## Paths to use for ingress rules
-    ##
-    paths: []
-    # - /
-
-    ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
-    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
-    # pathType: ImplementationSpecific
-
-    ## Secret name containing the TLS certificate for alertmanager per replica ingress
-    ## Secret must be manually created in the namespace
-    tlsSecretName: ""
-
-    ## Separated secret for each per replica Ingress. Can be used together with cert-manager
-    ##
-    tlsSecretPerReplica:
-      enabled: false
-      ## Final form of the secret for each per replica ingress is
-      ## {{ tlsSecretPerReplica.prefix }}-{{ $replicaNumber }}
-      ##
-      prefix: "alertmanager"
-
   ## Configuration for Alertmanager service
   ##
   service:
@@ -1228,86 +507,17 @@
     ##
     externalTrafficPolicy: Cluster
 
-    ## If you want to make sure that connections from a particular client are passed to the same Pod each time
-    ## Accepts 'ClientIP' or 'None'
-    ##
-    sessionAffinity: None
-
-    ## If you want to modify the ClientIP sessionAffinity timeout
-    ## The value must be >0 && <=86400(for 1 day) if ServiceAffinity == "ClientIP"
-    ##
-    sessionAffinityConfig:
-      clientIP:
-        timeoutSeconds: 10800
-
-    ## Service type
-    ##
-    type: ClusterIP
-
-  ## Configuration for creating a separate Service for each statefulset Alertmanager replica
-  ##
-  servicePerReplica:
-    enabled: false
-    annotations: {}
-
-    ## Port for Alertmanager Service per replica to listen on
-    ##
-    port: 9093
-
-    ## To be used with a proxy extraContainer port
-    targetPort: 9093
-
-    ## Port to expose on each node
-    ## Only used if servicePerReplica.type is 'NodePort'
-    ##
-    nodePort: 30904
-
-    ## Loadbalancer source IP ranges
-    ## Only used if servicePerReplica.type is "LoadBalancer"
-    loadBalancerSourceRanges: []
-
-    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
-    ##
-    externalTrafficPolicy: Cluster
-
     ## Service type
     ##
     type: ClusterIP
 
-  ## Configuration for creating a ServiceMonitor for AlertManager
+  ## If true, create a serviceMonitor for alertmanager
   ##
   serviceMonitor:
-    ## If true, a ServiceMonitor will be created for the AlertManager service.
-    ##
-    selfMonitor: true
-
     ## Scrape interval. If not set, the Prometheus default scrape interval is used.
     ##
     interval: ""
-
-    ## Additional labels
-    ##
-    additionalLabels: {}
-
-    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
-    ##
-    sampleLimit: 0
-
-    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
-    ##
-    targetLimit: 0
-
-    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelLimit: 0
-
-    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelNameLengthLimit: 0
-
-    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelValueLengthLimit: 0
+    selfMonitor: true
 
     ## proxyUrl: URL of a proxy that should be used for scraping.
     ##
@@ -1316,10 +526,6 @@
     ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.
     scheme: ""
 
-    ## enableHttp2: Whether to enable HTTP2.
-    ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint
-    enableHttp2: true
-
     ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.
     ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig
     tlsConfig: {}
@@ -1345,12 +551,6 @@
     #   replacement: $1
     #   action: replace
 
-    ## Additional Endpoints
-    ##
-    additionalEndpoints: []
-    # - port: oauth-metrics
-    #   path: /metrics
-
   ## Settings affecting alertmanagerSpec
   ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerspec
   ##
@@ -1397,39 +597,13 @@
 
     ## AlertmanagerConfigs to be selected to merge and configure Alertmanager with.
     ##
-    alertmanagerConfigSelector: {}
-    ## Example which selects all alertmanagerConfig resources
-    ## with label "alertconfig" with values any of "example-config" or "example-config-2"
-    # alertmanagerConfigSelector:
-    #   matchExpressions:
-    #     - key: alertconfig
-    #       operator: In
-    #       values:
-    #         - example-config
-    #         - example-config-2
-    #
-    ## Example which selects all alertmanagerConfig resources with label "role" set to "example-config"
-    # alertmanagerConfigSelector:
-    #   matchLabels:
-    #     role: example-config
-
-    ## Namespaces to be selected for AlertmanagerConfig discovery. If nil, only check own namespace.
-    ##
-    alertmanagerConfigNamespaceSelector: {}
-    ## Example which selects all namespaces
-    ## with label "alertmanagerconfig" with values any of "example-namespace" or "example-namespace-2"
-    # alertmanagerConfigNamespaceSelector:
-    #   matchExpressions:
-    #     - key: alertmanagerconfig
-    #       operator: In
-    #       values:
-    #         - example-namespace
-    #         - example-namespace-2
-
-    ## Example which selects all namespaces with label "alertmanagerconfig" set to "enabled"
-    # alertmanagerConfigNamespaceSelector:
-    #   matchLabels:
-    #     alertmanagerconfig: enabled
+    alertmanagerConfigSelector:
+      # default ignores resources created by Rancher Monitoring
+      matchExpressions:
+        - key: release
+          operator: NotIn
+          values:
+            - rancher-monitoring
 
     ## AlermanagerConfig to be used as top level configuration
     ##
@@ -1708,7 +882,7 @@
     namespace: cattle-dashboards
     # Whether to create the default namespace as a Helm managed namespace or use an existing namespace
     # If false, the defaultDashboards.namespace will be created as a Helm managed namespace
-    useExistingNamespace: false
+    useExistingNamespace: true
     # Whether the Helm managed namespace created by this chart should be left behind on a Helm uninstall
     # If you place other dashboards in this namespace, then they will be deleted on a helm uninstall
     # Ignore if useExistingNamespace is true
@@ -1982,1769 +1156,15 @@
   testFramework:
     enabled: false
 
-## Flag to disable all the kubernetes component scrapers
-##
-kubernetesServiceMonitors:
-  enabled: true
-
-## Component scraping the kube api server
-##
-kubeApiServer:
-  enabled: true
-  tlsConfig:
-    serverName: kubernetes
-    insecureSkipVerify: false
-  serviceMonitor:
-    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
-    ##
-    interval: ""
-
-    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
-    ##
-    sampleLimit: 0
-
-    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
-    ##
-    targetLimit: 0
-
-    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelLimit: 0
-
-    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelNameLengthLimit: 0
-
-    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelValueLengthLimit: 0
-
-    ## proxyUrl: URL of a proxy that should be used for scraping.
-    ##
-    proxyUrl: ""
-
-    jobLabel: component
-    selector:
-      matchLabels:
-        component: apiserver
-        provider: kubernetes
-
-    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    metricRelabelings:
-      # Drop excessively noisy apiserver buckets.
-      - action: drop
-        regex: apiserver_request_duration_seconds_bucket;(0.15|0.2|0.3|0.35|0.4|0.45|0.6|0.7|0.8|0.9|1.25|1.5|1.75|2|3|3.5|4|4.5|6|7|8|9|15|25|40|50)
-        sourceLabels:
-          - __name__
-          - le
-    # - action: keep
-    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
-    #   sourceLabels: [__name__]
-
-    ## RelabelConfigs to apply to samples before scraping
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    relabelings: []
-    # - sourceLabels:
-    #     - __meta_kubernetes_namespace
-    #     - __meta_kubernetes_service_name
-    #     - __meta_kubernetes_endpoint_port_name
-    #   action: keep
-    #   regex: default;kubernetes;https
-    # - targetLabel: __address__
-    #   replacement: kubernetes.default.svc:443
-
-    ## Additional labels
-    ##
-    additionalLabels: {}
-    #  foo: bar
-
-## Component scraping the kubelet and kubelet-hosted cAdvisor
-##
-kubelet:
-  enabled: true
-  namespace: kube-system
-
-  serviceMonitor:
-    ## Attach metadata to discovered targets. Requires Prometheus v2.45 for endpoints created by the operator.
-    ##
-    attachMetadata:
-      node: false
-
-    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
-    ##
-    interval: ""
-
-    ## If true, Prometheus use (respect) labels provided by exporter.
-    ##
-    honorLabels: true
-
-    ## If true, Prometheus ingests metrics with timestamp provided by exporter. If false, Prometheus ingests metrics with timestamp of scrape.
-    ##
-    honorTimestamps: true
-
-    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
-    ##
-    sampleLimit: 0
-
-    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
-    ##
-    targetLimit: 0
-
-    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelLimit: 0
-
-    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelNameLengthLimit: 0
-
-    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelValueLengthLimit: 0
-
-    ## proxyUrl: URL of a proxy that should be used for scraping.
-    ##
-    proxyUrl: ""
-
-    ## Enable scraping the kubelet over https. For requirements to enable this see
-    ## https://github.com/prometheus-operator/prometheus-operator/issues/926
-    ##
-    https: true
-
-    ## Skip TLS certificate validation when scraping.
-    ## This is enabled by default because kubelet serving certificate deployed by kubeadm is by default self-signed
-    ## ref: https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#kubelet-serving-certs
-    ##
-    insecureSkipVerify: true
-
-    ## Enable scraping /metrics/cadvisor from kubelet's service
-    ##
-    cAdvisor: true
-
-    ## Enable scraping /metrics/probes from kubelet's service
-    ##
-    probes: true
-
-    ## Enable scraping /metrics/resource from kubelet's service
-    ## This is disabled by default because container metrics are already exposed by cAdvisor
-    ##
-    resource: false
-    # From kubernetes 1.18, /metrics/resource/v1alpha1 renamed to /metrics/resource
-    resourcePath: "/metrics/resource/v1alpha1"
-
-    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    cAdvisorMetricRelabelings:
-      # Drop less useful container CPU metrics.
-      - sourceLabels: [__name__]
-        action: drop
-        regex: 'container_cpu_(cfs_throttled_seconds_total|load_average_10s|system_seconds_total|user_seconds_total)'
-      # Drop less useful container / always zero filesystem metrics.
-      - sourceLabels: [__name__]
-        action: drop
-        regex: 'container_fs_(io_current|io_time_seconds_total|io_time_weighted_seconds_total|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)'
-      # Drop less useful / always zero container memory metrics.
-      - sourceLabels: [__name__]
-        action: drop
-        regex: 'container_memory_(mapped_file|swap)'
-      # Drop less useful container process metrics.
-      - sourceLabels: [__name__]
-        action: drop
-        regex: 'container_(file_descriptors|tasks_state|threads_max)'
-      # Drop container spec metrics that overlap with kube-state-metrics.
-      - sourceLabels: [__name__]
-        action: drop
-        regex: 'container_spec.*'
-      # Drop cgroup metrics with no pod.
-      - sourceLabels: [id, pod]
-        action: drop
-        regex: '.+;'
-    # - sourceLabels: [__name__, image]
-    #   separator: ;
-    #   regex: container_([a-z_]+);
-    #   replacement: $1
-    #   action: drop
-    # - sourceLabels: [__name__]
-    #   separator: ;
-    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)
-    #   replacement: $1
-    #   action: drop
-
-    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    probesMetricRelabelings: []
-    # - sourceLabels: [__name__, image]
-    #   separator: ;
-    #   regex: container_([a-z_]+);
-    #   replacement: $1
-    #   action: drop
-    # - sourceLabels: [__name__]
-    #   separator: ;
-    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)
-    #   replacement: $1
-    #   action: drop
-
-    ## RelabelConfigs to apply to samples before scraping
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    ## metrics_path is required to match upstream rules and charts
-    cAdvisorRelabelings:
-      - action: replace
-        sourceLabels: [__metrics_path__]
-        targetLabel: metrics_path
-    # - sourceLabels: [__meta_kubernetes_pod_node_name]
-    #   separator: ;
-    #   regex: ^(.*)$
-    #   targetLabel: nodename
-    #   replacement: $1
-    #   action: replace
-
-    ## RelabelConfigs to apply to samples before scraping
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    probesRelabelings:
-      - action: replace
-        sourceLabels: [__metrics_path__]
-        targetLabel: metrics_path
-    # - sourceLabels: [__meta_kubernetes_pod_node_name]
-    #   separator: ;
-    #   regex: ^(.*)$
-    #   targetLabel: nodename
-    #   replacement: $1
-    #   action: replace
-
-    ## RelabelConfigs to apply to samples before scraping
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    resourceRelabelings:
-      - action: replace
-        sourceLabels: [__metrics_path__]
-        targetLabel: metrics_path
-    # - sourceLabels: [__meta_kubernetes_pod_node_name]
-    #   separator: ;
-    #   regex: ^(.*)$
-    #   targetLabel: nodename
-    #   replacement: $1
-    #   action: replace
-
-    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    metricRelabelings: []
-    # - sourceLabels: [__name__, image]
-    #   separator: ;
-    #   regex: container_([a-z_]+);
-    #   replacement: $1
-    #   action: drop
-    # - sourceLabels: [__name__]
-    #   separator: ;
-    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)
-    #   replacement: $1
-    #   action: drop
-
-    ## RelabelConfigs to apply to samples before scraping
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    ## metrics_path is required to match upstream rules and charts
-    relabelings:
-      - action: replace
-        sourceLabels: [__metrics_path__]
-        targetLabel: metrics_path
-    # - sourceLabels: [__meta_kubernetes_pod_node_name]
-    #   separator: ;
-    #   regex: ^(.*)$
-    #   targetLabel: nodename
-    #   replacement: $1
-    #   action: replace
-
-    ## Additional labels
-    ##
-    additionalLabels: {}
-    #  foo: bar
-
-## Component scraping the kube controller manager
-##
-kubeControllerManager:
-  enabled: false
-
-  ## If your kube controller manager is not deployed as a pod, specify IPs it can be found on
-  ##
-  endpoints: []
-  # - 10.141.4.22
-  # - 10.141.4.23
-  # - 10.141.4.24
-
-  ## If using kubeControllerManager.endpoints only the port and targetPort are used
-  ##
-  service:
-    enabled: true
-    ## If null or unset, the value is determined dynamically based on target Kubernetes version due to change
-    ## of default port in Kubernetes 1.22.
-    ##
-    port: null
-    targetPort: null
-    ipDualStack:
-      enabled: false
-      ipFamilies: ["IPv6", "IPv4"]
-      ipFamilyPolicy: "PreferDualStack"
-    # selector:
-    #   component: kube-controller-manager
-
-  serviceMonitor:
-    enabled: true
-    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
-    ##
-    interval: ""
-
-    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
-    ##
-    sampleLimit: 0
-
-    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
-    ##
-    targetLimit: 0
-
-    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelLimit: 0
-
-    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelNameLengthLimit: 0
-
-    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelValueLengthLimit: 0
-
-    ## proxyUrl: URL of a proxy that should be used for scraping.
-    ##
-    proxyUrl: ""
-
-    ## port: Name of the port the metrics will be scraped from
-    ##
-    port: http-metrics
-
-    jobLabel: jobLabel
-    selector: {}
-    #  matchLabels:
-    #    component: kube-controller-manager
-
-    ## Enable scraping kube-controller-manager over https.
-    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks.
-    ## If null or unset, the value is determined dynamically based on target Kubernetes version.
-    ##
-    https: null
-
-    # Skip TLS certificate validation when scraping
-    insecureSkipVerify: null
-
-    # Name of the server to use when validating TLS certificate
-    serverName: null
-
-    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    metricRelabelings: []
-    # - action: keep
-    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
-    #   sourceLabels: [__name__]
-
-    ## RelabelConfigs to apply to samples before scraping
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    relabelings: []
-    # - sourceLabels: [__meta_kubernetes_pod_node_name]
-    #   separator: ;
-    #   regex: ^(.*)$
-    #   targetLabel: nodename
-    #   replacement: $1
-    #   action: replace
-
-    ## Additional labels
-    ##
-    additionalLabels: {}
-    #  foo: bar
-
-## Component scraping coreDns. Use either this or kubeDns
-##
-coreDns:
-  enabled: true
-  service:
-    enabled: true
-    port: 9153
-    targetPort: 9153
-    ipDualStack:
-      enabled: false
-      ipFamilies: ["IPv6", "IPv4"]
-      ipFamilyPolicy: "PreferDualStack"
-    # selector:
-    #   k8s-app: kube-dns
-  serviceMonitor:
-    enabled: true
-    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
-    ##
-    interval: ""
-
-    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
-    ##
-    sampleLimit: 0
-
-    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
-    ##
-    targetLimit: 0
-
-    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelLimit: 0
-
-    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelNameLengthLimit: 0
-
-    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelValueLengthLimit: 0
-
-    ## proxyUrl: URL of a proxy that should be used for scraping.
-    ##
-    proxyUrl: ""
-
-    ## port: Name of the port the metrics will be scraped from
-    ##
-    port: http-metrics
-
-    jobLabel: jobLabel
-    selector: {}
-    #  matchLabels:
-    #    k8s-app: kube-dns
-
-    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    metricRelabelings: []
-    # - action: keep
-    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
-    #   sourceLabels: [__name__]
-
-    ## RelabelConfigs to apply to samples before scraping
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    relabelings: []
-    # - sourceLabels: [__meta_kubernetes_pod_node_name]
-    #   separator: ;
-    #   regex: ^(.*)$
-    #   targetLabel: nodename
-    #   replacement: $1
-    #   action: replace
-
-    ## Additional labels
-    ##
-    additionalLabels: {}
-    #  foo: bar
-
-## Component scraping kubeDns. Use either this or coreDns
-##
-kubeDns:
-  enabled: false
-  service:
-    dnsmasq:
-      port: 10054
-      targetPort: 10054
-    skydns:
-      port: 10055
-      targetPort: 10055
-    ipDualStack:
-      enabled: false
-      ipFamilies: ["IPv6", "IPv4"]
-      ipFamilyPolicy: "PreferDualStack"
-    # selector:
-    #   k8s-app: kube-dns
-  serviceMonitor:
-    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
-    ##
-    interval: ""
-
-    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
-    ##
-    sampleLimit: 0
-
-    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
-    ##
-    targetLimit: 0
-
-    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelLimit: 0
-
-    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelNameLengthLimit: 0
-
-    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelValueLengthLimit: 0
-
-    ## proxyUrl: URL of a proxy that should be used for scraping.
-    ##
-    proxyUrl: ""
-
-    jobLabel: jobLabel
-    selector: {}
-    #  matchLabels:
-    #    k8s-app: kube-dns
-
-    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    metricRelabelings: []
-    # - action: keep
-    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
-    #   sourceLabels: [__name__]
-
-    ## RelabelConfigs to apply to samples before scraping
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    relabelings: []
-    # - sourceLabels: [__meta_kubernetes_pod_node_name]
-    #   separator: ;
-    #   regex: ^(.*)$
-    #   targetLabel: nodename
-    #   replacement: $1
-    #   action: replace
-
-    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    dnsmasqMetricRelabelings: []
-    # - action: keep
-    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
-    #   sourceLabels: [__name__]
-
-    ## RelabelConfigs to apply to samples before scraping
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    dnsmasqRelabelings: []
-    # - sourceLabels: [__meta_kubernetes_pod_node_name]
-    #   separator: ;
-    #   regex: ^(.*)$
-    #   targetLabel: nodename
-    #   replacement: $1
-    #   action: replace
-
-    ## Additional labels
-    ##
-    additionalLabels: {}
-    #  foo: bar
-
-## Component scraping etcd
-##
-kubeEtcd:
-  enabled: false
-
-  ## If your etcd is not deployed as a pod, specify IPs it can be found on
-  ##
-  endpoints: []
-  # - 10.141.4.22
-  # - 10.141.4.23
-  # - 10.141.4.24
-
-  ## Etcd service. If using kubeEtcd.endpoints only the port and targetPort are used
-  ##
-  service:
-    enabled: true
-    port: 2381
-    targetPort: 2381
-    ipDualStack:
-      enabled: false
-      ipFamilies: ["IPv6", "IPv4"]
-      ipFamilyPolicy: "PreferDualStack"
-    # selector:
-    #   component: etcd
-
-  ## Configure secure access to the etcd cluster by loading a secret into prometheus and
-  ## specifying security configuration below. For example, with a secret named etcd-client-cert
-  ##
-  ## serviceMonitor:
-  ##   scheme: https
-  ##   insecureSkipVerify: false
-  ##   serverName: localhost
-  ##   caFile: /etc/prometheus/secrets/etcd-client-cert/etcd-ca
-  ##   certFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client
-  ##   keyFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client-key
-  ##
-  serviceMonitor:
-    enabled: true
-    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
-    ##
-    interval: ""
-
-    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
-    ##
-    sampleLimit: 0
-
-    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
-    ##
-    targetLimit: 0
-
-    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelLimit: 0
-
-    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelNameLengthLimit: 0
-
-    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelValueLengthLimit: 0
-
-    ## proxyUrl: URL of a proxy that should be used for scraping.
-    ##
-    proxyUrl: ""
-    scheme: http
-    insecureSkipVerify: false
-    serverName: ""
-    caFile: ""
-    certFile: ""
-    keyFile: ""
-
-    ## port: Name of the port the metrics will be scraped from
-    ##
-    port: http-metrics
-
-    jobLabel: jobLabel
-    selector: {}
-    #  matchLabels:
-    #    component: etcd
-
-    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    metricRelabelings: []
-    # - action: keep
-    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
-    #   sourceLabels: [__name__]
-
-    ## RelabelConfigs to apply to samples before scraping
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    relabelings: []
-    # - sourceLabels: [__meta_kubernetes_pod_node_name]
-    #   separator: ;
-    #   regex: ^(.*)$
-    #   targetLabel: nodename
-    #   replacement: $1
-    #   action: replace
-
-    ## Additional labels
-    ##
-    additionalLabels: {}
-    #  foo: bar
-
-## Component scraping kube scheduler
-##
-kubeScheduler:
-  enabled: false
-
-  ## If your kube scheduler is not deployed as a pod, specify IPs it can be found on
-  ##
-  endpoints: []
-  # - 10.141.4.22
-  # - 10.141.4.23
-  # - 10.141.4.24
-
-  ## If using kubeScheduler.endpoints only the port and targetPort are used
-  ##
-  service:
-    enabled: true
-    ## If null or unset, the value is determined dynamically based on target Kubernetes version due to change
-    ## of default port in Kubernetes 1.23.
-    ##
-    port: null
-    targetPort: null
-    ipDualStack:
-      enabled: false
-      ipFamilies: ["IPv6", "IPv4"]
-      ipFamilyPolicy: "PreferDualStack"
-    # selector:
-    #   component: kube-scheduler
-
-  serviceMonitor:
-    enabled: true
-    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
-    ##
-    interval: ""
-
-    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
-    ##
-    sampleLimit: 0
-
-    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
-    ##
-    targetLimit: 0
-
-    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelLimit: 0
-
-    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelNameLengthLimit: 0
-
-    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelValueLengthLimit: 0
-
-    ## proxyUrl: URL of a proxy that should be used for scraping.
-    ##
-    proxyUrl: ""
-    ## Enable scraping kube-scheduler over https.
-    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks.
-    ## If null or unset, the value is determined dynamically based on target Kubernetes version.
-    ##
-    https: null
-
-    ## port: Name of the port the metrics will be scraped from
-    ##
-    port: http-metrics
-
-    jobLabel: jobLabel
-    selector: {}
-    #  matchLabels:
-    #    component: kube-scheduler
-
-    ## Skip TLS certificate validation when scraping
-    insecureSkipVerify: null
-
-    ## Name of the server to use when validating TLS certificate
-    serverName: null
-
-    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    metricRelabelings: []
-    # - action: keep
-    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
-    #   sourceLabels: [__name__]
-
-    ## RelabelConfigs to apply to samples before scraping
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    relabelings: []
-    # - sourceLabels: [__meta_kubernetes_pod_node_name]
-    #   separator: ;
-    #   regex: ^(.*)$
-    #   targetLabel: nodename
-    #   replacement: $1
-    #   action: replace
-
-    ## Additional labels
-    ##
-    additionalLabels: {}
-    #  foo: bar
-
-## Component scraping kube proxy
-##
-kubeProxy:
-  enabled: false
-
-  ## If your kube proxy is not deployed as a pod, specify IPs it can be found on
-  ##
-  endpoints: []
-  # - 10.141.4.22
-  # - 10.141.4.23
-  # - 10.141.4.24
-
-  service:
-    enabled: true
-    port: 10249
-    targetPort: 10249
-    ipDualStack:
-      enabled: false
-      ipFamilies: ["IPv6", "IPv4"]
-      ipFamilyPolicy: "PreferDualStack"
-    # selector:
-    #   k8s-app: kube-proxy
-
-  serviceMonitor:
-    enabled: true
-    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
-    ##
-    interval: ""
-
-    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
-    ##
-    sampleLimit: 0
-
-    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
-    ##
-    targetLimit: 0
-
-    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelLimit: 0
-
-    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelNameLengthLimit: 0
-
-    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelValueLengthLimit: 0
-
-    ## proxyUrl: URL of a proxy that should be used for scraping.
-    ##
-    proxyUrl: ""
-
-    ## port: Name of the port the metrics will be scraped from
-    ##
-    port: http-metrics
-
-    jobLabel: jobLabel
-    selector: {}
-    #  matchLabels:
-    #    k8s-app: kube-proxy
-
-    ## Enable scraping kube-proxy over https.
-    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks
-    ##
-    https: false
-
-    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    metricRelabelings: []
-    # - action: keep
-    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
-    #   sourceLabels: [__name__]
-
-    ## RelabelConfigs to apply to samples before scraping
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-    ##
-    relabelings: []
-    # - action: keep
-    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
-    #   sourceLabels: [__name__]
-
-    ## Additional labels
-    ##
-    additionalLabels: {}
-    #  foo: bar
-
-## Component scraping kube state metrics
-##
-kubeStateMetrics:
-  enabled: true
-
-## Configuration for kube-state-metrics subchart
-##
-kube-state-metrics:
-  namespaceOverride: ""
-  rbac:
-    create: true
-  releaseLabel: true
-  prometheus:
-    monitor:
-      enabled: true
-
-      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
-      ##
-      interval: ""
-
-      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
-      ##
-      sampleLimit: 0
-
-      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
-      ##
-      targetLimit: 0
-
-      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-      ##
-      labelLimit: 0
-
-      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-      ##
-      labelNameLengthLimit: 0
-
-      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-      ##
-      labelValueLengthLimit: 0
-
-      ## Scrape Timeout. If not set, the Prometheus default scrape timeout is used.
-      ##
-      scrapeTimeout: ""
-
-      ## proxyUrl: URL of a proxy that should be used for scraping.
-      ##
-      proxyUrl: ""
-
-      # Keep labels from scraped data, overriding server-side labels
-      ##
-      honorLabels: true
-
-      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
-      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-      ##
-      metricRelabelings: []
-      # - action: keep
-      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
-      #   sourceLabels: [__name__]
-
-      ## RelabelConfigs to apply to samples before scraping
-      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-      ##
-      relabelings: []
-      # - sourceLabels: [__meta_kubernetes_pod_node_name]
-      #   separator: ;
-      #   regex: ^(.*)$
-      #   targetLabel: nodename
-      #   replacement: $1
-      #   action: replace
-
-  selfMonitor:
-    enabled: false
-
-## Deploy node exporter as a daemonset to all nodes
-##
-nodeExporter:
-  enabled: true
-  operatingSystems:
-    linux:
-      enabled: true
-    darwin:
-      enabled: true
-
-  ## ForceDeployDashboard Create dashboard configmap even if nodeExporter deployment has been disabled
-  ##
-  forceDeployDashboards: false
-
-## Configuration for prometheus-node-exporter subchart
-##
-prometheus-node-exporter:
-  namespaceOverride: ""
-  podLabels:
-    ## Add the 'node-exporter' label to be used by serviceMonitor to match standard common usage in rules and grafana dashboards
-    ##
-    jobLabel: node-exporter
-  releaseLabel: true
-  extraArgs:
-    - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
-    - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
-  service:
-    portName: http-metrics
-    ipDualStack:
-      enabled: false
-      ipFamilies: ["IPv6", "IPv4"]
-      ipFamilyPolicy: "PreferDualStack"
-  prometheus:
-    monitor:
-      enabled: true
-
-      jobLabel: jobLabel
-
-      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
-      ##
-      interval: ""
-
-      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
-      ##
-      sampleLimit: 0
-
-      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
-      ##
-      targetLimit: 0
-
-      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-      ##
-      labelLimit: 0
-
-      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-      ##
-      labelNameLengthLimit: 0
-
-      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-      ##
-      labelValueLengthLimit: 0
-
-      ## How long until a scrape request times out. If not set, the Prometheus default scape timeout is used.
-      ##
-      scrapeTimeout: ""
-
-      ## proxyUrl: URL of a proxy that should be used for scraping.
-      ##
-      proxyUrl: ""
-
-      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
-      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-      ##
-      metricRelabelings: []
-      # - sourceLabels: [__name__]
-      #   separator: ;
-      #   regex: ^node_mountstats_nfs_(event|operations|transport)_.+
-      #   replacement: $1
-      #   action: drop
-
-      ## RelabelConfigs to apply to samples before scraping
-      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
-      ##
-      relabelings: []
-      # - sourceLabels: [__meta_kubernetes_pod_node_name]
-      #   separator: ;
-      #   regex: ^(.*)$
-      #   targetLabel: nodename
-      #   replacement: $1
-      #   action: replace
-  rbac:
-    ## If true, create PSPs for node-exporter
-    ##
-    pspEnabled: false
-
-## Manages Prometheus and Alertmanager components
-##
-prometheusOperator:
-  enabled: true
-
-  ## Use '{{ template "kube-prometheus-stack.fullname" . }}-operator' by default
-  fullnameOverride: ""
-
-  ## Number of old replicasets to retain ##
-  ## The default value is 10, 0 will garbage-collect old replicasets ##
-  revisionHistoryLimit: 10
-
-  ## Strategy of the deployment
-  ##
-  strategy: {}
-
-  ## Prometheus-Operator v0.39.0 and later support TLS natively.
-  ##
-  tls:
-    enabled: true
-    # Value must match version names from https://golang.org/pkg/crypto/tls/#pkg-constants
-    tlsMinVersion: VersionTLS13
-    # The default webhook port is 10250 in order to work out-of-the-box in GKE private clusters and avoid adding firewall rules.
-    internalPort: 10250
-
-  ## Admission webhook support for PrometheusRules resources added in Prometheus Operator 0.30 can be enabled to prevent incorrectly formatted
-  ## rules from making their way into prometheus and potentially preventing the container from starting
-  admissionWebhooks:
-    ## Valid values: Fail, Ignore, IgnoreOnInstallOnly
-    ## IgnoreOnInstallOnly - If Release.IsInstall returns "true", set "Ignore" otherwise "Fail"
-    failurePolicy: ""
-    ## The default timeoutSeconds is 10 and the maximum value is 30.
-    timeoutSeconds: 10
-    enabled: true
-    ## A PEM encoded CA bundle which will be used to validate the webhook's server certificate.
-    ## If unspecified, system trust roots on the apiserver are used.
-    caBundle: ""
-    ## If enabled, generate a self-signed certificate, then patch the webhook configurations with the generated data.
-    ## On chart upgrades (or if the secret exists) the cert will not be re-generated. You can use this to provide your own
-    ## certs ahead of time if you wish.
-    ##
-    annotations: {}
-    #   argocd.argoproj.io/hook: PreSync
-    #   argocd.argoproj.io/hook-delete-policy: HookSucceeded
-
-    namespaceSelector: {}
-    objectSelector: {}
-
-
-    deployment:
-      enabled: false
-
-      ## Number of replicas
-      ##
-      replicas: 1
-
-      ## Strategy of the deployment
-      ##
-      strategy: {}
-
-      # Ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/
-      podDisruptionBudget: {}
-        # maxUnavailable: 1
-        # minAvailable: 1
-
-      ## Number of old replicasets to retain ##
-      ## The default value is 10, 0 will garbage-collect old replicasets ##
-      revisionHistoryLimit: 10
-
-      ## Prometheus-Operator v0.39.0 and later support TLS natively.
-      ##
-      tls:
-        enabled: true
-        # Value must match version names from https://golang.org/pkg/crypto/tls/#pkg-constants
-        tlsMinVersion: VersionTLS13
-        # The default webhook port is 10250 in order to work out-of-the-box in GKE private clusters and avoid adding firewall rules.
-        internalPort: 10250
-
-      ## Service account for Prometheus Operator Webhook to use.
-      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
-      ##
-      serviceAccount:
-        automountServiceAccountToken: false
-        create: true
-        name: ""
-
-      ## Configuration for Prometheus operator Webhook service
-      ##
-      service:
-        annotations: {}
-        labels: {}
-        clusterIP: ""
-        ipDualStack:
-          enabled: false
-          ipFamilies: ["IPv6", "IPv4"]
-          ipFamilyPolicy: "PreferDualStack"
-
-        ## Port to expose on each node
-        ## Only used if service.type is 'NodePort'
-        ##
-        nodePort: 31080
-
-        nodePortTls: 31443
-
-        ## Additional ports to open for Prometheus operator Webhook service
-        ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#multi-port-services
-        ##
-        additionalPorts: []
-
-        ## Loadbalancer IP
-        ## Only use if service.type is "LoadBalancer"
-        ##
-        loadBalancerIP: ""
-        loadBalancerSourceRanges: []
-
-        ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
-        ##
-        externalTrafficPolicy: Cluster
-
-        ## Service type
-        ## NodePort, ClusterIP, LoadBalancer
-        ##
-        type: ClusterIP
-
-        ## List of IP addresses at which the Prometheus server service is available
-        ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
-        ##
-        externalIPs: []
-
-      # ## Labels to add to the operator webhook deployment
-      # ##
-      labels: {}
-
-      ## Annotations to add to the operator webhook deployment
-      ##
-      annotations: {}
-
-      ## Labels to add to the operator webhook pod
-      ##
-      podLabels: {}
-
-      ## Annotations to add to the operator webhook pod
-      ##
-      podAnnotations: {}
-
-      ## Assign a PriorityClassName to pods if set
-      # priorityClassName: ""
-
-      ## Define Log Format
-      # Use logfmt (default) or json logging
-      # logFormat: logfmt
-
-      ## Decrease log verbosity to errors only
-      # logLevel: error
-
-      ## Prometheus-operator webhook image
-      ##
-      image:
-        registry: quay.io
-        repository: rancher/mirrored-prometheus-operator-admission-webhook
-        # if not set appVersion field from Chart.yaml is used
-        tag: v0.75.1
-        sha: ""
-        pullPolicy: IfNotPresent
-
-      ## Define Log Format
-      # Use logfmt (default) or json logging
-      # logFormat: logfmt
-
-      ## Decrease log verbosity to errors only
-      # logLevel: error
-
-
-      ## Liveness probe
-      ##
-      livenessProbe:
-        enabled: true
-        failureThreshold: 3
-        initialDelaySeconds: 30
-        periodSeconds: 10
-        successThreshold: 1
-        timeoutSeconds: 1
-
-      ## Readiness probe
-      ##
-      readinessProbe:
-        enabled: true
-        failureThreshold: 3
-        initialDelaySeconds: 5
-        periodSeconds: 10
-        successThreshold: 1
-        timeoutSeconds: 1
-
-      ## Resource limits & requests
-      ##
-      resources: {}
-      # limits:
-      #   cpu: 200m
-      #   memory: 200Mi
-      # requests:
-      #   cpu: 100m
-      #   memory: 100Mi
-
-      # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),
-      # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working
-      ##
-      hostNetwork: false
-
-      ## Define which Nodes the Pods are scheduled on.
-      ## ref: https://kubernetes.io/docs/user-guide/node-selection/
-      ##
-      nodeSelector: {}
-
-      ## Tolerations for use with node taints
-      ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
-      ##
-      tolerations: []
-      # - key: "key"
-      #   operator: "Equal"
-      #   value: "value"
-      #   effect: "NoSchedule"
-
-      ## Assign custom affinity rules to the prometheus operator
-      ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
-      ##
-      affinity: {}
-        # nodeAffinity:
-        #   requiredDuringSchedulingIgnoredDuringExecution:
-        #     nodeSelectorTerms:
-        #     - matchExpressions:
-        #       - key: kubernetes.io/e2e-az-name
-        #         operator: In
-        #         values:
-        #         - e2e-az1
-      #         - e2e-az2
-      dnsConfig: {}
-        # nameservers:
-        #   - 1.2.3.4
-        # searches:
-        #   - ns1.svc.cluster-domain.example
-        #   - my.dns.search.suffix
-        # options:
-        #   - name: ndots
-        #     value: "2"
-        #   - name: edns0
-      securityContext:
-        fsGroup: 65534
-        runAsGroup: 65534
-        runAsNonRoot: true
-        runAsUser: 65534
-        seccompProfile:
-          type: RuntimeDefault
-
-      ## Container-specific security context configuration
-      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
-      ##
-      containerSecurityContext:
-        allowPrivilegeEscalation: false
-        readOnlyRootFilesystem: true
-        capabilities:
-          drop:
-            - ALL
-
-      ## If false then the user will opt out of automounting API credentials.
-      ##
-      automountServiceAccountToken: true
-
-    patch:
-      enabled: true
-      image:
-        repository: rancher/mirrored-ingress-nginx-kube-webhook-certgen
-        tag: v1.4.3
-        sha: ""
-        pullPolicy: IfNotPresent
-      resources: {}
-      ## Provide a priority class name to the webhook patching job
-      ##
-      priorityClassName: ""
-      ttlSecondsAfterFinished: 60
-      annotations: {}
-      #   argocd.argoproj.io/hook: PreSync
-      #   argocd.argoproj.io/hook-delete-policy: HookSucceeded
-      podAnnotations: {}
-      nodeSelector: {}
-      affinity: {}
-      tolerations: []
-
-      ## SecurityContext holds pod-level security attributes and common container settings.
-      ## This defaults to non root user with uid 2000 and gid 2000. *v1.PodSecurityContext  false
-      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
-      ##
-      securityContext:
-        runAsGroup: 2000
-        runAsNonRoot: true
-        runAsUser: 2000
-        seccompProfile:
-          type: RuntimeDefault
-      ## Service account for Prometheus Operator Webhook Job Patch to use.
-      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
-      ##
-      serviceAccount:
-        create: true
-        automountServiceAccountToken: true
-
-    # Security context for create job container
-    createSecretJob:
-      securityContext:
-        allowPrivilegeEscalation: false
-        readOnlyRootFilesystem: true
-        capabilities:
-          drop:
-          - ALL
-
-      # Security context for patch job container
-    patchWebhookJob:
-      securityContext:
-        allowPrivilegeEscalation: false
-        readOnlyRootFilesystem: true
-        capabilities:
-          drop:
-          - ALL
-
-    # Use certmanager to generate webhook certs
-    certManager:
-      enabled: false
-      # self-signed root certificate
-      rootCert:
-        duration: ""  # default to be 5y
-      admissionCert:
-        duration: ""  # default to be 1y
-      # issuerRef:
-      #   name: "issuer"
-      #   kind: "ClusterIssuer"
-
-  ## Namespaces to scope the interaction of the Prometheus Operator and the apiserver (allow list).
-  ## This is mutually exclusive with denyNamespaces. Setting this to an empty object will disable the configuration
-  ##
-  namespaces: {}
-    # releaseNamespace: true
-    # additional:
-    # - kube-system
-
-  ## Namespaces not to scope the interaction of the Prometheus Operator (deny list).
-  ##
-  denyNamespaces: []
-
-  ## Filter namespaces to look for prometheus-operator custom resources
-  ##
-  alertmanagerInstanceNamespaces: []
-  alertmanagerConfigNamespaces: []
-  prometheusInstanceNamespaces: []
-  thanosRulerInstanceNamespaces: []
-
-  ## The clusterDomain value will be added to the cluster.peer option of the alertmanager.
-  ## Without this specified option cluster.peer will have value alertmanager-monitoring-alertmanager-0.alertmanager-operated:9094 (default value)
-  ## With this specified option cluster.peer will have value alertmanager-monitoring-alertmanager-0.alertmanager-operated.namespace.svc.cluster-domain:9094
-  ##
-  # clusterDomain: "cluster.local"
-
-  networkPolicy:
-    ## Enable creation of NetworkPolicy resources.
-    ##
-    enabled: false
-
-    ## Flavor of the network policy to use.
-    #  Can be:
-    #  * kubernetes for networking.k8s.io/v1/NetworkPolicy
-    #  * cilium     for cilium.io/v2/CiliumNetworkPolicy
-    flavor: kubernetes
-
-    # cilium:
-    #   egress:
-
-    ## match labels used in selector
-    # matchLabels: {}
-
-  ## Service account for Prometheus Operator to use.
-  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
-  ##
-  serviceAccount:
-    create: true
-    name: ""
-    automountServiceAccountToken: true
-
-  ## Configuration for Prometheus operator service
-  ##
-  service:
-    annotations: {}
-    labels: {}
-    clusterIP: ""
-    ipDualStack:
-      enabled: false
-      ipFamilies: ["IPv6", "IPv4"]
-      ipFamilyPolicy: "PreferDualStack"
-
-  ## Port to expose on each node
-  ## Only used if service.type is 'NodePort'
-  ##
-    nodePort: 30080
-
-    nodePortTls: 30443
-
-  ## Additional ports to open for Prometheus operator service
-  ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#multi-port-services
-  ##
-    additionalPorts: []
-
-  ## Loadbalancer IP
-  ## Only use if service.type is "LoadBalancer"
-  ##
-    loadBalancerIP: ""
-    loadBalancerSourceRanges: []
-
-    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
-    ##
-    externalTrafficPolicy: Cluster
-
-  ## Service type
-  ## NodePort, ClusterIP, LoadBalancer
-  ##
-    type: ClusterIP
-
-    ## List of IP addresses at which the Prometheus server service is available
-    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
-    ##
-    externalIPs: []
-
-  # ## Labels to add to the operator deployment
-  # ##
-  labels: {}
-
-  ## Annotations to add to the operator deployment
-  ##
-  annotations: {}
-
-  ## Labels to add to the operator pod
-  ##
-  podLabels: {}
-
-  ## Annotations to add to the operator pod
-  ##
-  podAnnotations: {}
-
-  ## Assign a PriorityClassName to pods if set
-  # priorityClassName: ""
-
-  ## Define Log Format
-  # Use logfmt (default) or json logging
-  # logFormat: logfmt
-
-  ## Decrease log verbosity to errors only
-  # logLevel: error
-
-  kubeletService:
-    ## If true, the operator will create and maintain a service for scraping kubelets
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/helm/prometheus-operator/README.md
-    ##
-    enabled: true
-    namespace: kube-system
-    selector: ""
-    ## Use '{{ template "kube-prometheus-stack.fullname" . }}-kubelet' by default
-    name: ""
-
-  ## Create a servicemonitor for the operator
-  ##
-  serviceMonitor:
-    ## If true, create a serviceMonitor for prometheus operator
-    ##
-    selfMonitor: true
-
-    ## Labels for ServiceMonitor
-    additionalLabels: {}
-
-    ## Scrape interval. If not set, the Prometheus default scrape interval is used.
-    ##
-    interval: ""
-
-    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
-    ##
-    sampleLimit: 0
-
-    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
-    ##
-    targetLimit: 0
-
-    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelLimit: 0
-
-    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelNameLengthLimit: 0
-
-    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelValueLengthLimit: 0
-
-    ## Scrape timeout. If not set, the Prometheus default scrape timeout is used.
-    scrapeTimeout: ""
-
-    ## Metric relabel configs to apply to samples before ingestion.
-    ##
-    metricRelabelings: []
-    # - action: keep
-    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
-    #   sourceLabels: [__name__]
-
-    #   relabel configs to apply to samples before ingestion.
-    ##
-    relabelings: []
-    # - sourceLabels: [__meta_kubernetes_pod_node_name]
-    #   separator: ;
-    #   regex: ^(.*)$
-    #   targetLabel: nodename
-    #   replacement: $1
-    #   action: replace
-
-  ## Resource limits & requests
-  ##
-  resources: {}
-  # limits:
-  #   cpu: 200m
-  #   memory: 200Mi
-  # requests:
-  #   cpu: 100m
-  #   memory: 100Mi
-
-  ## Operator Environment
-  ##  env:
-  ##    VARIABLE: value
-  env:
-    GOGC: "30"
-
-  # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),
-  # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working
-  ##
-  hostNetwork: false
-
-  ## Define which Nodes the Pods are scheduled on.
-  ## ref: https://kubernetes.io/docs/user-guide/node-selection/
-  ##
-  nodeSelector: {}
-
-  ## Tolerations for use with node taints
-  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
-  ##
-  tolerations: []
-  # - key: "key"
-  #   operator: "Equal"
-  #   value: "value"
-  #   effect: "NoSchedule"
-
-  ## Assign custom affinity rules to the prometheus operator
-  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
-  ##
-  affinity: {}
-    # nodeAffinity:
-    #   requiredDuringSchedulingIgnoredDuringExecution:
-    #     nodeSelectorTerms:
-    #     - matchExpressions:
-    #       - key: kubernetes.io/e2e-az-name
-    #         operator: In
-    #         values:
-    #         - e2e-az1
-    #         - e2e-az2
-  dnsConfig: {}
-    # nameservers:
-    #   - 1.2.3.4
-    # searches:
-    #   - ns1.svc.cluster-domain.example
-    #   - my.dns.search.suffix
-    # options:
-    #   - name: ndots
-    #     value: "2"
-  #   - name: edns0
-  securityContext:
-    fsGroup: 65534
-    runAsGroup: 65534
-    runAsNonRoot: true
-    runAsUser: 65534
-    seccompProfile:
-      type: RuntimeDefault
-
-  ## Container-specific security context configuration
-  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
-  ##
-  containerSecurityContext:
-    allowPrivilegeEscalation: false
-    readOnlyRootFilesystem: true
-    capabilities:
-      drop:
-      - ALL
-
-  # Enable vertical pod autoscaler support for prometheus-operator
-  verticalPodAutoscaler:
-    enabled: false
-
-    # Recommender responsible for generating recommendation for the object.
-    # List should be empty (then the default recommender will generate the recommendation)
-    # or contain exactly one recommender.
-    # recommenders:
-    # - name: custom-recommender-performance
-
-    # List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory
-    controlledResources: []
-    # Specifies which resource values should be controlled: RequestsOnly or RequestsAndLimits.
-    # controlledValues: RequestsAndLimits
-
-    # Define the max allowed resources for the pod
-    maxAllowed: {}
-    # cpu: 200m
-    # memory: 100Mi
-    # Define the min allowed resources for the pod
-    minAllowed: {}
-    # cpu: 200m
-    # memory: 100Mi
-
-    updatePolicy:
-      # Specifies minimal number of replicas which need to be alive for VPA Updater to attempt pod eviction
-      # minReplicas: 1
-      # Specifies whether recommended updates are applied when a Pod is started and whether recommended updates
-      # are applied during the life of a Pod. Possible values are "Off", "Initial", "Recreate", and "Auto".
-      updateMode: Auto
-
-  ## Prometheus-operator image
-  ##
-  image:
-    repository: rancher/mirrored-prometheus-operator-prometheus-operator
-    tag: v0.75.1
-    sha: ""
-    pullPolicy: IfNotPresent
-
-  ## Prometheus image to use for prometheuses managed by the operator
-  ##
-  # prometheusDefaultBaseImage: prometheus/prometheus
-
-  ## Prometheus image registry to use for prometheuses managed by the operator
-  ##
-  # prometheusDefaultBaseImageRegistry: quay.io
-
-  ## Alertmanager image to use for alertmanagers managed by the operator
-  ##
-  # alertmanagerDefaultBaseImage: prometheus/alertmanager
-
-  ## Alertmanager image registry to use for alertmanagers managed by the operator
-  ##
-  # alertmanagerDefaultBaseImageRegistry: quay.io
-
-  ## Prometheus-config-reloader
-  ##
-  prometheusConfigReloader:
-    image:
-      repository: rancher/mirrored-prometheus-operator-prometheus-config-reloader
-      tag: v0.75.1
-      sha: ""
-
-    # add prometheus config reloader liveness and readiness probe. Default: false
-    enableProbe: false
-
-    # resource config for prometheusConfigReloader
-    resources: {}
-      # requests:
-      #   cpu: 200m
-      #   memory: 50Mi
-      # limits:
-      #   cpu: 200m
-      #   memory: 50Mi
-
-  ## Thanos side-car image when configured
-  ##
-  thanosImage:
-    repository: rancher/mirrored-thanos-thanos
-    tag: v0.35.1
-    sha: ""
-
-  ## Set a Label Selector to filter watched prometheus and prometheusAgent
-  ##
-  prometheusInstanceSelector: ""
-
-  ## Set a Label Selector to filter watched alertmanager
-  ##
-  alertmanagerInstanceSelector: ""
-
-  ## Set a Label Selector to filter watched thanosRuler
-  thanosRulerInstanceSelector: ""
-
-  ## Set a Field Selector to filter watched secrets
-  ##
-  secretFieldSelector: "type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1"
-
-  ## If false then the user will opt out of automounting API credentials.
-  ##
-  automountServiceAccountToken: true
-
-  ## Additional volumes
-  ##
-  extraVolumes: []
-
-  ## Additional volume mounts
-  ##
-  extraVolumeMounts: []
-
 ## Deploy a Prometheus instance
 ##
 prometheus:
   enabled: true
 
-  ## Toggle prometheus into agent mode
-  ## Note many of features described below (e.g. rules, query, alerting, remote read, thanos) will not work in agent mode.
-  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/designs/prometheus-agent.md
-  ##
-  agentMode: false
-
   ## Annotations for Prometheus
   ##
   annotations: {}
 
-  ## Configure network policy for the prometheus
-  networkPolicy:
-    enabled: false
-
-    ## Flavor of the network policy to use.
-    #  Can be:
-    #  * kubernetes for networking.k8s.io/v1/NetworkPolicy
-    #  * cilium     for cilium.io/v2/CiliumNetworkPolicy
-    flavor: kubernetes
-
-    # cilium:
-    #   endpointSelector:
-    #   egress:
-    #   ingress:
-
-    # egress:
-    # - {}
-    # ingress:
-    # - {}
-    # podSelector:
-    #   matchLabels:
-    #     app: prometheus
-
   ## Service account for Prometheuses to use.
   ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
   ##
@@ -3752,51 +1172,6 @@
     create: true
     name: ""
     annotations: {}
-    automountServiceAccountToken: true
-
-  # Service for thanos service discovery on sidecar
-  # Enable this can make Thanos Query can use
-  # `--store=dnssrv+_grpc._tcp.${kube-prometheus-stack.fullname}-thanos-discovery.${namespace}.svc.cluster.local` to discovery
-  # Thanos sidecar on prometheus nodes
-  # (Please remember to change ${kube-prometheus-stack.fullname} and ${namespace}. Not just copy and paste!)
-  thanosService:
-    enabled: false
-    annotations: {}
-    labels: {}
-
-    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
-    ##
-    externalTrafficPolicy: Cluster
-
-    ## Service type
-    ##
-    type: ClusterIP
-
-    ## Service dual stack
-    ##
-    ipDualStack:
-      enabled: false
-      ipFamilies: ["IPv6", "IPv4"]
-      ipFamilyPolicy: "PreferDualStack"
-
-    ## gRPC port config
-    portName: grpc
-    port: 10901
-    targetPort: "grpc"
-
-    ## HTTP port config (for metrics)
-    httpPortName: http
-    httpPort: 10902
-    targetHttpPort: "http"
-
-    ## ClusterIP to assign
-    # Default is to make this a headless service ("None")
-    clusterIP: "None"
-
-    ## Port to expose on each node, if service type is NodePort
-    ##
-    nodePort: 30901
-    httpNodePort: 30902
 
   # ServiceMonitor to scrape Sidecar metrics
   # Needs thanosService to be enabled as well
@@ -3915,47 +1290,7 @@
     ## Ref: https://kubernetes.io/docs/reference/kubernetes-api/service-resources/service-v1/#ServiceSpec
     publishNotReadyAddresses: false
 
-    ## If you want to make sure that connections from a particular client are passed to the same Pod each time
-    ## Accepts 'ClientIP' or 'None'
-    ##
-    sessionAffinity: None
-
-    ## If you want to modify the ClientIP sessionAffinity timeout
-    ## The value must be >0 && <=86400(for 1 day) if ServiceAffinity == "ClientIP"
-    ##
-    sessionAffinityConfig:
-      clientIP:
-        timeoutSeconds: 10800
-
-  ## Configuration for creating a separate Service for each statefulset Prometheus replica
-  ##
-  servicePerReplica:
-    enabled: false
-    annotations: {}
-
-    ## Port for Prometheus Service per replica to listen on
-    ##
-    port: 9090
-
-    ## To be used with a proxy extraContainer port
-    targetPort: 9090
-
-    ## Port to expose on each node
-    ## Only used if servicePerReplica.type is 'NodePort'
-    ##
-    nodePort: 30091
-
-    ## Loadbalancer source IP ranges
-    ## Only used if servicePerReplica.type is "LoadBalancer"
-    loadBalancerSourceRanges: []
-
-    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
-    ##
-    externalTrafficPolicy: Cluster
-
-    ## Service type
-    ##
-    type: ClusterIP
+    sessionAffinity: ""
 
     ## Service dual stack
     ##
@@ -3972,46 +1307,6 @@
     minAvailable: 1
     maxUnavailable: ""
 
-  # Ingress exposes thanos sidecar outside the cluster
-  thanosIngress:
-    enabled: false
-
-    # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
-    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
-    # ingressClassName: nginx
-
-    annotations: {}
-    labels: {}
-    servicePort: 10901
-
-    ## Port to expose on each node
-    ## Only used if service.type is 'NodePort'
-    ##
-    nodePort: 30901
-
-    ## Hosts must be provided if Ingress is enabled.
-    ##
-    hosts: []
-      # - thanos-gateway.domain.com
-
-    ## Paths to use for ingress rules
-    ##
-    paths: []
-    # - /
-
-    ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
-    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
-    # pathType: ImplementationSpecific
-
-    ## TLS configuration for Thanos Ingress
-    ## Secret must be manually created in the namespace
-    ##
-    tls: []
-    # - secretName: thanos-gateway-tls
-    #   hosts:
-    #   - thanos-gateway.domain.com
-    #
-
   ## ExtraSecret can be used to store various data in an extra secret
   ## (use it for example to store hashed basic auth credentials)
   extraSecret:
@@ -4060,89 +1355,17 @@
       #   hosts:
       #     - prometheus.example.com
 
-  ## Configuration for creating an Ingress that will map to each Prometheus replica service
-  ## prometheus.servicePerReplica must be enabled
-  ##
-  ingressPerReplica:
-    enabled: false
-
-    # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
-    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
-    # ingressClassName: nginx
-
-    annotations: {}
-    labels: {}
-
-    ## Final form of the hostname for each per replica ingress is
-    ## {{ ingressPerReplica.hostPrefix }}-{{ $replicaNumber }}.{{ ingressPerReplica.hostDomain }}
-    ##
-    ## Prefix for the per replica ingress that will have `-$replicaNumber`
-    ## appended to the end
-    hostPrefix: ""
-    ## Domain that will be used for the per replica ingress
-    hostDomain: ""
-
-    ## Paths to use for ingress rules
-    ##
-    paths: []
-    # - /
-
-    ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
-    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
-    # pathType: ImplementationSpecific
-
-    ## Secret name containing the TLS certificate for Prometheus per replica ingress
-    ## Secret must be manually created in the namespace
-    tlsSecretName: ""
-
-    ## Separated secret for each per replica Ingress. Can be used together with cert-manager
-    ##
-    tlsSecretPerReplica:
-      enabled: false
-      ## Final form of the secret for each per replica ingress is
-      ## {{ tlsSecretPerReplica.prefix }}-{{ $replicaNumber }}
-      ##
-      prefix: "prometheus"
-
   ## Configure additional options for default pod security policy for Prometheus
   ## ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/
   podSecurityPolicy:
     allowedCapabilities: []
-    allowedHostPaths: []
     volumes: []
 
   serviceMonitor:
-    ## If true, create a serviceMonitor for prometheus
-    ##
-    selfMonitor: true
-
     ## Scrape interval. If not set, the Prometheus default scrape interval is used.
     ##
     interval: ""
-
-    ## Additional labels
-    ##
-    additionalLabels: {}
-
-    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
-    ##
-    sampleLimit: 0
-
-    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
-    ##
-    targetLimit: 0
-
-    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelLimit: 0
-
-    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelNameLengthLimit: 0
-
-    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
-    ##
-    labelValueLengthLimit: 0
+    selfMonitor: true
 
     ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.
     scheme: ""
@@ -4170,12 +1393,6 @@
     #   replacement: $1
     #   action: replace
 
-    ## Additional Endpoints
-    ##
-    additionalEndpoints: []
-    # - port: oauth-metrics
-    #   path: /metrics
-
   ## Settings affecting prometheusSpec
   ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#prometheusspec
   ##
@@ -4203,10 +1420,6 @@
     ##
     apiserverConfig: {}
 
-    ## Allows setting additional arguments for the Prometheus container
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.Prometheus
-    additionalArgs: []
-
     ## Interval between consecutive scrapes.
     ## Defaults to 30s.
     ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/release-0.44/pkg/prometheus/promcfg.go#L180-L183
@@ -4215,7 +1428,7 @@
 
     ## Number of seconds to wait for target to respond before erroring
     ##
-    # scrapeTimeout: ""
+    scrapeTimeout: ""
 
     ## List of scrape classes to expose to scraping objects such as
     ## PodMonitors, ServiceMonitors, Probes and ScrapeConfigs.
@@ -4229,7 +1442,7 @@
 
     ## Interval between consecutive evaluations.
     ##
-    evaluationInterval: "30s"
+    evaluationInterval: "1m"
 
     ## ListenLocal makes the Prometheus server listen on loopback, so that it does not bind against the Pod IP.
     ##
@@ -4241,10 +1454,6 @@
     ##
     enableAdminAPI: false
 
-    ## Sets version of Prometheus overriding the Prometheus version as derived
-    ## from the image tag. Useful in cases where the tag does not follow semver v2.
-    version: ""
-
     ## WebTLSConfig defines the TLS parameters for HTTPS
     ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#webtlsconfig
     web: {}
@@ -4312,14 +1521,6 @@
     ##
     enableRemoteWriteReceiver: false
 
-    ## Name of the external label used to denote replica name
-    ##
-    replicaExternalLabelName: ""
-
-    ## If true, the Operator won't add the external label used to denote replica name
-    ##
-    replicaExternalLabelNameClear: false
-
     ## Name of the external label used to denote Prometheus instance name
     ##
     prometheusExternalLabelName: ""
@@ -4354,13 +1555,6 @@
     ##
     query: {}
 
-    ## If nil, select own namespace. Namespaces to be selected for PrometheusRules discovery.
-    ruleNamespaceSelector: {}
-    ## Example which selects PrometheusRules in namespaces with label "prometheus" set to "somelabel"
-    # ruleNamespaceSelector:
-    #   matchLabels:
-    #     prometheus: somelabel
-
     ## If true, a nil or {} value for prometheus.prometheusSpec.ruleSelector will cause the
     ## prometheus resource to be created with selectors based on values in the helm deployment,
     ## which will also match the PrometheusRule resources created
@@ -4370,21 +1564,13 @@
     ## PrometheusRules to be selected for target discovery.
     ## If {}, select all PrometheusRules
     ##
-    ruleSelector: {}
-    ## Example which select all PrometheusRules resources
-    ## with label "prometheus" with values any of "example-rules" or "example-rules-2"
-    # ruleSelector:
-    #   matchExpressions:
-    #     - key: prometheus
-    #       operator: In
-    #       values:
-    #         - example-rules
-    #         - example-rules-2
-    #
-    ## Example which select all PrometheusRules resources with label "role" set to "example-rules"
-    # ruleSelector:
-    #   matchLabels:
-    #     role: example-rules
+    ruleSelector:
+      # default ignores resources created by Rancher Monitoring
+      matchExpressions:
+        - key: release
+          operator: NotIn
+          values:
+            - rancher-monitoring
 
     ## If true, a nil or {} value for prometheus.prometheusSpec.serviceMonitorSelector will cause the
     ## prometheus resource to be created with selectors based on values in the helm deployment,
@@ -4395,19 +1581,13 @@
     ## ServiceMonitors to be selected for target discovery.
     ## If {}, select all ServiceMonitors
     ##
-    serviceMonitorSelector: {}
-    ## Example which selects ServiceMonitors with label "prometheus" set to "somelabel"
-    # serviceMonitorSelector:
-    #   matchLabels:
-    #     prometheus: somelabel
-
-    ## Namespaces to be selected for ServiceMonitor discovery.
-    ##
-    serviceMonitorNamespaceSelector: {}
-    ## Example which selects ServiceMonitors in namespaces with label "prometheus" set to "somelabel"
-    # serviceMonitorNamespaceSelector:
-    #   matchLabels:
-    #     prometheus: somelabel
+    serviceMonitorSelector:
+      # default ignores resources created by Rancher Monitoring
+      matchExpressions:
+        - key: release
+          operator: NotIn
+          values:
+            - rancher-monitoring
 
     ## If true, a nil or {} value for prometheus.prometheusSpec.podMonitorSelector will cause the
     ## prometheus resource to be created with selectors based on values in the helm deployment,
@@ -4418,18 +1598,13 @@
     ## PodMonitors to be selected for target discovery.
     ## If {}, select all PodMonitors
     ##
-    podMonitorSelector: {}
-    ## Example which selects PodMonitors with label "prometheus" set to "somelabel"
-    # podMonitorSelector:
-    #   matchLabels:
-    #     prometheus: somelabel
-
-    ## If nil, select own namespace. Namespaces to be selected for PodMonitor discovery.
-    podMonitorNamespaceSelector: {}
-    ## Example which selects PodMonitor in namespaces with label "prometheus" set to "somelabel"
-    # podMonitorNamespaceSelector:
-    #   matchLabels:
-    #     prometheus: somelabel
+    podMonitorSelector:
+      # default ignores resources created by Rancher Monitoring
+      matchExpressions:
+        - key: release
+          operator: NotIn
+          values:
+            - rancher-monitoring
 
     ## If true, a nil or {} value for prometheus.prometheusSpec.probeSelector will cause the
     ## prometheus resource to be created with selectors based on values in the helm deployment,
@@ -4440,40 +1615,13 @@
     ## Probes to be selected for target discovery.
     ## If {}, select all Probes
     ##
-    probeSelector: {}
-    ## Example which selects Probes with label "prometheus" set to "somelabel"
-    # probeSelector:
-    #   matchLabels:
-    #     prometheus: somelabel
-
-    ## If nil, select own namespace. Namespaces to be selected for Probe discovery.
-    probeNamespaceSelector: {}
-    ## Example which selects Probe in namespaces with label "prometheus" set to "somelabel"
-    # probeNamespaceSelector:
-    #   matchLabels:
-    #     prometheus: somelabel
-
-    ## If true, a nil or {} value for prometheus.prometheusSpec.scrapeConfigSelector will cause the
-    ## prometheus resource to be created with selectors based on values in the helm deployment,
-    ## which will also match the scrapeConfigs created
-    ##
-    scrapeConfigSelectorNilUsesHelmValues: true
-
-    ## scrapeConfigs to be selected for target discovery.
-    ## If {}, select all scrapeConfigs
-    ##
-    scrapeConfigSelector: {}
-    ## Example which selects scrapeConfigs with label "prometheus" set to "somelabel"
-    # scrapeConfigSelector:
-    #   matchLabels:
-    #     prometheus: somelabel
-
-    ## If nil, select own namespace. Namespaces to be selected for scrapeConfig discovery.
-    scrapeConfigNamespaceSelector: {}
-    ## Example which selects scrapeConfig in namespaces with label "prometheus" set to "somelabel"
-    # scrapeConfigNamespaceSelector:
-    #   matchLabels:
-    #     prometheus: somelabel
+    probeSelector:
+      # default ignores resources created by Rancher Monitoring
+      matchExpressions:
+        - key: release
+          operator: NotIn
+          values:
+            - rancher-monitoring
 
     ## How long to retain metrics
     ##
@@ -4481,12 +1629,7 @@
 
     ## Maximum size of metrics
     ##
-    retentionSize: ""
-
-    ## Allow out-of-order/out-of-bounds samples ingested into Prometheus for a specified duration
-    ## See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#tsdb
-    tsdb:
-      outOfOrderTimeWindow: 0s
+    retentionSize: "50GiB"
 
     ## Enable compression of the write-ahead log using Snappy.
     ##
@@ -4556,13 +1699,6 @@
     #         - e2e-az1
     #         - e2e-az2
 
-    ## The remote_read spec configuration for Prometheus.
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#remotereadspec
-    remoteRead: []
-    # - url: http://remote1/read
-    ## additionalRemoteRead is appended to remoteRead
-    additionalRemoteRead: []
-
     ## The remote_write spec configuration for Prometheus.
     ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#remotewritespec
     remoteWrite: []
@@ -4583,6 +1719,8 @@
         memory: 750Mi
         cpu: 750m
 
+    storage:
+      enabled: false
     ## Prometheus StorageSpec for persistent data
     ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md
     ##
@@ -4850,15 +1988,6 @@
     ## of Prometheus >= 2.16.0. For more details, see the Prometheus docs (https://prometheus.io/docs/guides/query-log/)
     queryLogFile: false
 
-    # Use to set global sample_limit for Prometheus. This act as default SampleLimit for ServiceMonitor or/and PodMonitor.
-    # Set to 'false' to disable global sample_limit. or set to a number to override the default value.
-    sampleLimit: false
-
-    # EnforcedKeepDroppedTargetsLimit defines on the number of targets dropped by relabeling that will be kept in memory.
-    # The value overrides any spec.keepDroppedTargets set by ServiceMonitor, PodMonitor, Probe objects unless spec.keepDroppedTargets
-    # is greater than zero and less than spec.enforcedKeepDroppedTargets. 0 means no limit.
-    enforcedKeepDroppedTargets: 0
-
     ## EnforcedSampleLimit defines global limit on number of scraped samples that will be accepted. This overrides any SampleLimit
     ## set per ServiceMonitor or/and PodMonitor. It is meant to be used by admins to enforce the SampleLimit to keep overall
     ## number of samples/series under the desired limit. Note that if SampleLimit is lower that value will be taken instead.
@@ -4870,7 +1999,6 @@
     ## if either value is zero, in which case the non-zero value will be used. If both values are zero, no limit is enforced.
     enforcedTargetLimit: false
 
-
     ## Per-scrape limit on number of labels that will be accepted for a sample. If more than this number of labels are present
     ## post metric-relabeling, the entire scrape will be treated as failed. 0 means no limit. Only valid in Prometheus versions
     ## 2.27.0 and newer.
@@ -4908,25 +2036,6 @@
     #      - a1.app.local
     #      - b1.app.local
 
-    ## TracingConfig configures tracing in Prometheus.
-    ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#prometheustracingconfig
-    tracingConfig: {}
-
-    ## Additional configuration which is not covered by the properties above. (passed through tpl)
-    additionalConfig: {}
-
-    ## Additional configuration which is not covered by the properties above.
-    ## Useful, if you need advanced templating inside alertmanagerSpec.
-    ## Otherwise, use prometheus.prometheusSpec.additionalConfig (passed through tpl)
-    additionalConfigString: ""
-
-    ## Defines the maximum time that the `prometheus` container's startup probe
-    ## will wait before being considered failed. The startup probe will return
-    ## success after the WAL replay is complete. If set, the value should be
-    ## greater than 60 (seconds). Otherwise it will be equal to 900 seconds (15
-    ## minutes).
-    maximumStartupDurationSeconds: 0
-
   additionalRulesForClusterRole: []
   #  - apiGroups: [ "" ]
   #    resources:
@@ -5253,12 +2362,6 @@
     #   replacement: $1
     #   action: replace
 
-    ## Additional Endpoints
-    ##
-    additionalEndpoints: []
-    # - port: oauth-metrics
-    #   path: /metrics
-
   ## Settings affecting thanosRulerpec
   ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#thanosrulerspec
   ##
@@ -5341,24 +2444,16 @@
 
     ## AlertmanagerConfig define configuration for connecting to alertmanager.
     ## Only available with Thanos v0.10.0 and higher. Maps to the alertmanagers.config Thanos Ruler arg.
-    alertmanagersConfig:
-      # use existing secret, if configured, alertmanagersConfig.secret will not be used
-      existingSecret: {}
-        # name: ""
-        # key: ""
-      # will render render alertmanagersConfig secret data and configure it to be used by Thanos Ruler custom resource, ignored when alertmanagersConfig.existingSecret is set
-      # https://thanos.io/tip/components/rule.md/#alertmanager
-      secret: {}
-        # alertmanagers:
-        # - api_version: v2
-        #   http_config:
-        #     basic_auth:
-        #       username: some_user
-        #       password: some_pass
-        #   static_configs:
-        #     - alertmanager.thanos.io
-        #   scheme: http
-        #   timeout: 10s
+    alertmanagersConfig: {}
+    #   - api_version: v2
+    #     http_config:
+    #       basic_auth:
+    #         username: some_user
+    #         password: some_pass
+    #     static_configs:
+    #       - alertmanager.thanos.io
+    #     scheme: http
+    #     timeout: 10s
 
     ## DEPRECATED. Define URLs to send alerts to Alertmanager. For Thanos v0.10.0 and higher, alertmanagersConfig should be used instead.
     ## Note: this field will be ignored if alertmanagersConfig is specified. Maps to the alertmanagers.url Thanos Ruler arg.
@@ -5373,22 +2468,13 @@
     ##
     routePrefix: /
 
-    ## ObjectStorageConfig configures object storage in Thanos
-    objectStorageConfig:
-      # use existing secret, if configured, objectStorageConfig.secret will not be used
-      existingSecret: {}
-        # name: ""
-        # key: ""
-      # will render objectStorageConfig secret data and configure it to be used by Thanos Ruler custom resource, ignored when objectStorageConfig.existingSecret is set
-      # https://thanos.io/tip/thanos/storage.md/#s3
-      secret: {}
-        # type: S3
-        # config:
-        #   bucket: ""
-        #   endpoint: ""
-        #   region: ""
-        #   access_key: ""
-        #   secret_key: ""
+    ## ObjectStorageConfig configures object storage in Thanos. Alternative to
+    ## ObjectStorageConfigFile, and lower order priority.
+    objectStorageConfig: {}
+
+    ## ObjectStorageConfigFile specifies the path of the object storage configuration file.
+    ## When used alongside with ObjectStorageConfig, ObjectStorageConfigFile takes precedence.
+    objectStorageConfigFile: ""
 
     ## Labels by name to drop before sending to alertmanager
     ## Maps to the --alert.label-drop flag of thanos ruler.
@@ -5400,22 +2486,7 @@
 
     ## Define configuration for connecting to thanos query instances. If this is defined, the queryEndpoints field will be ignored.
     ## Maps to the query.config CLI argument. Only available with thanos v0.11.0 and higher.
-    queryConfig:
-      # use existing secret, if configured, queryConfig.secret will not be used
-      existingSecret: {}
-        # name: ""
-        # key: ""
-      # render queryConfig secret data and configure it to be used by Thanos Ruler custom resource, ignored when queryConfig.existingSecret is set
-      # https://thanos.io/tip/components/rule.md/#query-api
-      secret: {}
-        # - http_config:
-        #     basic_auth:
-        #       username: some_user
-        #       password: some_pass
-        #   static_configs:
-        #     - URL
-        #   scheme: http
-        #   timeout: 10s
+    queryConfig: {}
 
     ## Labels configure the external label pairs to ThanosRuler. A default replica
     ## label `thanos_ruler_replica` will be always added as a label with the value
@@ -5426,17 +2497,6 @@
     ##
     paused: false
 
-    ## Allows setting additional arguments for the ThanosRuler container
-    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#thanosruler
-    ##
-    additionalArgs: []
-      # - name: remote-write.config
-      #   value: |-
-      #     "remote_write":
-      #     - "name": "receiver-0"
-      #       "remote_timeout": "30s"
-      #       "url": "http://thanos-receiver-0.thanos-receiver:8081/api/v1/receive"
-
     ## Define which Nodes the Pods are scheduled on.
     ## ref: https://kubernetes.io/docs/user-guide/node-selection/
     ##
@@ -5504,8 +2564,6 @@
       runAsNonRoot: true
       runAsUser: 1000
       fsGroup: 2000
-      seccompProfile:
-        type: RuntimeDefault
 
     ## ListenLocal makes the ThanosRuler server listen on loopback, so that it does not bind against the Pod IP.
     ## Note this is only for the ThanosRuler UI, not the gossip communication.
@@ -5555,13 +2613,3 @@
 ## Setting to true produces cleaner resource names, but requires a data migration because the name of the persistent volume changes. Therefore this should only be set once on initial installation.
 ##
 cleanPrometheusOperatorObjectNames: false
-
-## Extra manifests to deploy as an array
-extraManifests: []
-  # - apiVersion: v1
-  #   kind: ConfigMap
-  #   metadata:
-  #   labels:
-  #     name: prometheus-extra
-  #   data:
-  #     extra-data: "value"
